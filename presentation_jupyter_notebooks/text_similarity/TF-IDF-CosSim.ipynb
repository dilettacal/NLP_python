{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation - Text Similarity (German texts)\n",
    "\n",
    "### Course in Content Management, Text and Search Technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from lxml.html import html_parser\n",
    "from spacy.lang.de import German\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "stopwords = stopwords.words(\"german\")\n",
    "stopwords.append(\"dass\")\n",
    "\n",
    "nlp = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a little corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['das rote auto hält an der roten ampel. das rote auto fährt zu schnell.',\n",
       " 'das schwarze auto wurde rot eingefärbt, weil die farbe rot lebendiger wirkt',\n",
       " 'das schwarze mofa fährt an der grünen ampel durch, hält aber am roten schild nicht und wird vom roten auto angefahren',\n",
       " 'morgen scheint die sonne und wir fahren ans meer']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS = [\n",
    "\"Das rote Auto hält an der roten Ampel. Das rote Auto fährt zu schnell.\", \n",
    "\"Das schwarze Auto wurde rot eingefärbt, weil die Farbe rot lebendiger wirkt\",\n",
    "\"Das schwarze Mofa fährt an der grünen Ampel durch, hält aber am roten Schild nicht und wird vom roten Auto angefahren\",\n",
    "\"Morgen scheint die Sonne und wir fahren ans Meer\"\n",
    "]\n",
    "\n",
    "CORPUS = [text.lower() for text in CORPUS]\n",
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['der',\n",
       " 'rote',\n",
       " 'auto',\n",
       " 'halten',\n",
       " 'an',\n",
       " 'der',\n",
       " 'rot',\n",
       " 'ampel',\n",
       " 'der',\n",
       " 'rote',\n",
       " 'schwarze',\n",
       " 'auto',\n",
       " 'fahren',\n",
       " 'zu',\n",
       " 'schnellen',\n",
       " 'der',\n",
       " 'schwarze',\n",
       " 'mofa',\n",
       " 'sein',\n",
       " 'langsam']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satz = \"Das rote Auto hält an der roten Ampel. Das rote, schwarze Auto fährt zu schnell. Das schwarze Mofa ist langsam\"\n",
    "satz = satz.lower()\n",
    "sent = nlp(satz)\n",
    "lemmas = [word.lemma_ for word in sent if not word.is_punct]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing failed for following adjectives/adverbs:\n",
    "\n",
    "- rote \n",
    "- schwarze \n",
    "\n",
    "Right forms should be: \"rot\" and \"schwarz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = CORPUS.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer with Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation is removed from text (string.punctuation is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_.lower().translate(remove_punct_dict) for token in doc if not token.is_punct or not token.like_num]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word frequency with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1\n",
    "stop = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_vectorizer = CountVectorizer(tokenizer=get_tokens, stop_words=stopwords, ngram_range=(start,stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 33 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vectorizer.fit_transform(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rote': 15, 'auto': 4, 'halten': 9, 'rot': 14, 'ampel': 1, '': 0, 'fahren': 6, 'schnellen': 18, 'schwarze': 19, 'einfärben': 5, 'farbe': 7, 'lebendig': 10, 'wirken': 21, 'mofa': 12, 'grün': 8, 'schild': 17, 'anfahren': 2, 'morgen': 13, 'scheinen': 16, 'sonne': 20, 'ans': 3, 'meer': 11}\n",
      "\n",
      "Anzahl der N-Gramme:  22\n"
     ]
    }
   ],
   "source": [
    "print(token_vectorizer.vocabulary_)\n",
    "print(\"\\nAnzahl der N-Gramme: \", len(token_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = token_vectorizer.transform(CORPUS).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 0 2 0 1 0 0 1 0 0 0 0 1 2 0 0 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 1 0 0 1 0 0 0 2 0 0 0 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0 1 1 0 0 1 0 2 0 0 1 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 22)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape #(3 = Dokumente, 43 = Wörter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute IDF and transform results to a TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a vector, whose components represent the IDF value for each word in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF- Score für die Wörter im Corpus: ´\n",
      " [1.22314355 1.51082562 1.91629073 1.91629073 1.22314355 1.91629073\n",
      " 1.22314355 1.91629073 1.91629073 1.51082562 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.22314355 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.51082562 1.91629073 1.91629073]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print(\"IDF- Score für die Wörter im Corpus: ´\\n\", tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF-Score für Wörter, die nur in 1 Dokument vorkommen: 1.6931471805599454\n",
      "IDF-Score für Wörter, die in 2 Dokumenten vorkommen: 1.2876820724517808\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "#Credits: https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/\n",
    "def idf(n,df):\n",
    "    result = math.log((n+1.0)/(df+1.0)) + 1\n",
    "    return result\n",
    "\n",
    "anzahl_doc = 3\n",
    "print (\"IDF-Score für Wörter, die nur in 1 Dokument vorkommen: \" + str(idf(anzahl_doc,1)))#1x \n",
    "print (\"IDF-Score für Wörter, die in 2 Dokumenten vorkommen: \" + str(idf(anzahl_doc,2)))#2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Matrix\n",
    "\n",
    "Creation of a TF-IDF matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39743237 0.24545402 0.         0.         0.39743237 0.\n",
      "  0.19871618 0.         0.         0.24545402 0.         0.\n",
      "  0.         0.         0.19871618 0.6226546  0.         0.\n",
      "  0.3113273  0.         0.         0.        ]\n",
      " [0.24011956 0.         0.         0.         0.24011956 0.37619368\n",
      "  0.         0.37619368 0.         0.         0.37619368 0.\n",
      "  0.         0.         0.48023911 0.         0.         0.\n",
      "  0.         0.29659542 0.         0.37619368]\n",
      " [0.21619279 0.26704111 0.33870779 0.         0.21619279 0.\n",
      "  0.21619279 0.         0.33870779 0.26704111 0.         0.\n",
      "  0.33870779 0.         0.43238559 0.         0.         0.33870779\n",
      "  0.         0.26704111 0.         0.        ]\n",
      " [0.         0.         0.         0.43003652 0.         0.\n",
      "  0.27448674 0.         0.         0.         0.         0.43003652\n",
      "  0.         0.43003652 0.         0.         0.43003652 0.\n",
      "  0.         0.         0.43003652 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "#The transform()-method multiplies the TF-matrix from above by the diagonal IDF-Matrix. \n",
    "#Then it divides the TF-IDF score by the euclidean norm.\n",
    "print (tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity\n",
    "\n",
    "The result matrix from the computation above is then multiplied by its transpose matrix. The result is a matrix containing distance values (which should represent the \"similarity\" among pair of terms in the text). As a result the first 3 documents are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.28629385 0.43181968 0.05454496]\n",
      " [0.28629385 1.         0.39067588 0.        ]\n",
      " [0.43181968 0.39067588 1.         0.05934205]\n",
      " [0.05454496 0.         0.05934205 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine-Similarity with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "A_sparse = sparse.csr_matrix(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.28629385 0.43181968 0.05454496]\n",
      " [0.28629385 1.         0.39067588 0.        ]\n",
      " [0.43181968 0.39067588 1.         0.05934205]\n",
      " [0.05454496 0.         0.05934205 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#print(A_sparse)\n",
    "\n",
    "similarities = cosine_similarity(A_sparse)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF-Vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TfidfVec = TfidfVectorizer(tokenizer=get_tokens, stop_words=stopwords, ngram_range=(1,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.13562957, 0.26402917, 0.02719766],\n",
       "       [0.13562957, 1.        , 0.18141068, 0.        ],\n",
       "       [0.26402917, 0.18141068, 1.        , 0.02900303],\n",
       "       [0.02719766, 0.        , 0.02900303, 1.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity(CORPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources / Inspiration: \n",
    "https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/\n",
    "\n",
    "http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
